
@article{meijering_cell_2012,
 title = {Cell {Segmentation}: 50 {Years} {Down} the {Road} [{Life} {Sciences}]},
 volume = {29},
 copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
 issn = {1053-5888},
 shorttitle = {Cell {Segmentation}},
 url = {http://ieeexplore.ieee.org/document/6279591/},
 doi = {10.1109/MSP.2012.2204190},
 number = {5},
 urldate = {2024-09-07},
 journal = {IEEE Signal Processing Magazine},
 author = {Meijering, Erik},
 month = sep,
 year = {2012},
 pages = {140--145},
}

@article{jyrki_selinummi_bright_2009,
 title = {Bright {Field} {Microscopy} as an {Alternative} to {Whole} {Cell} {Fluorescence} in {Automated} {Analysis} of {Macrophage} {Images}},
 volume = {4},
 doi = {10.1371/journal.pone.0007497},
 abstract = {Background
Fluorescence microscopy is the standard tool for detection and analysis of cellular phenomena. This technique, however, has a number of drawbacks such as the limited number of available fluorescent channels in microscopes, overlapping excitation and emission spectra of the stains, and phototoxicity.},
 number = {10},
 journal = {PLOS ONE},
 author = {{Jyrki Selinummi} and Selinummi, Jyrki and {Pekka Ruusuvuori} and Ruusuvuori, Pekka and {Irina Podolsky} and Podolsky, Irina and Podolsky, Irina and {Adrian Ozinsky} and Ozinsky, Adrian and {Elizabeth S. Gold} and Gold, Elizabeth S. and {Olli Yli‐Harja} and Yli-Harja, Olli and {Alan Aderem} and Aderem, Alan and {Ilya Shmulevich} and Shmulevich, Ilya},
 month = oct,
 year = {2009},
 doi = {10.1371/journal.pone.0007497},
 pmcid = {2760782},
 pmid = {19847301},
 note = {MAG ID: 1980189947
S2ID: 26af3e90694e8ed14e25a11f0492af6b2e39ca87},
}

@article{erick_moen_deep_2019,
 title = {Deep learning for cellular image analysis},
 volume = {16},
 doi = {10.1038/s41592-019-0403-1},
 abstract = {Recent advances in computer vision and machine learning underpin a collection of algorithms with an impressive ability to decipher the content of images. These deep learning algorithms are being applied to biological images and are transforming the analysis and interpretation of imaging data. These advances are positioned to render difficult analyses routine and to enable researchers to carry out new, previously impossible experiments. Here we review the intersection between deep learning and cellular image analysis and provide an overview of both the mathematical mechanics and the programming frameworks of deep learning that are pertinent to life scientists. We survey the field’s progress in four key applications: image classification, image segmentation, object tracking, and augmented microscopy. Last, we relay our labs’ experience with three key aspects of implementing deep learning in the laboratory: annotating training data, selecting and training a range of neural network architectures, and deploying solutions. We also highlight existing datasets and implementations for each surveyed application.},
 number = {12},
 journal = {Nature Methods},
 author = {{Erick Moen} and Moen, Erick K. and {Erick Moen} and {Dylan Bannon} and Bannon, Dylan and {Tetsuichi Kudo} and Kudo, Takamasa and {William D. Graf} and Graf, William and {Markus W. Covert} and Covert, Markus W. and {David Van Valen} and Van Valen, David},
 month = may,
 year = {2019},
 doi = {10.1038/s41592-019-0403-1},
 pmid = {31133758},
 note = {MAG ID: 2946901414},
 pages = {1233--1246},
}

@article{thorsten_falk_u-net_2019,
 title = {U-{Net}: deep learning for cell counting, detection, and morphometry},
 volume = {16},
 doi = {10.1038/s41592-018-0261-2},
 abstract = {U-Net is a generic deep-learning solution for frequently occurring quantification tasks such as cell detection and shape measurements in biomedical image data. We present an ImageJ plugin that enables non-machine-learning experts to analyze their data with U-Net on either a local computer or a remote server/cloud service. The plugin comes with pretrained models for single-cell segmentation and allows for U-Net to be adapted to new tasks on the basis of a few annotated samples. A user-friendly ImageJ plugin enables the application and training of U-Nets for deep-learning-based image segmentation, detection and classification tasks with minimal labeling requirements.},
 number = {1},
 journal = {Nature Methods},
 author = {{Thorsten Falk} and Falk, Thorsten and {Dominic Mai} and Mai, Dominic and {Robert Bensch} and Bensch, Robert and {Özgün Çiçek} and Çiçek, Özgün and {Ahmed Abdulkadir} and Abdulkadir, Ahmed and {Yassine Marrakchi} and Marrakchi, Yassine and {Anton Böhm} and Böhm, Anton and {Jan Deubner} and Deubner, Jan and {Zoë Jäckel} and Jäckel, Zoe and {Katharina Seiwald} and Seiwald, Katharina and {Alexander Dovzhenko} and Dovzhenko, Alexander and {Olaf Tietz} and Tietz, Olaf and {Cristina Dal Bosco} and Dal Bosco, Cristina and {Seán Walsh} and Walsh, Sean and {Deniz Saltukoglu} and Saltukoglu, Deniz and {Tuan Leng Tay} and Tay, Tuan Leng and {Marco Prinz} and Prinz, Marco and {Klaus Palme} and Palme, Klaus and {Klaus Palme} and {Matias Simons} and Simons, Matias and {Ilka Diester} and Diester, Ilka and {Thomas Brox} and Brox, Thomas and {Olaf Ronneberger} and Ronneberger, Olaf},
 month = jan,
 year = {2019},
 doi = {10.1038/s41592-018-0261-2},
 note = {MAG ID: 2900936384},
 pages = {67--70},
}

@article{ronneberger_u-net_2015,
 title = {U-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
 shorttitle = {U-{Net}},
 url = {http://arxiv.org/abs/1505.04597},
 abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
 urldate = {2022-02-10},
 journal = {arXiv:1505.04597 [cs]},
 author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
 month = may,
 year = {2015},
 note = {arXiv: 1505.04597},
 keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{ho_denoising_2020,
 title = {Denoising {Diffusion} {Probabilistic} {Models}},
 url = {http://arxiv.org/abs/2006.11239},
 abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion},
 urldate = {2023-08-07},
 publisher = {arXiv},
 author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
 month = dec,
 year = {2020},
 note = {arXiv:2006.11239 [cs, stat]},
 keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{rajaram_simucell_2012,
 title = {{SimuCell}: a flexible framework for creating synthetic microscopy images},
 volume = {9},
 copyright = {http://www.springer.com/tdm},
 issn = {1548-7091, 1548-7105},
 shorttitle = {{SimuCell}},
 url = {https://www.nature.com/articles/nmeth.2096},
 doi = {10.1038/nmeth.2096},
 language = {en},
 number = {7},
 urldate = {2024-09-07},
 journal = {Nature Methods},
 author = {Rajaram, Satwik and Pavie, Benjamin and Hac, Nicholas E F and Altschuler, Steven J and Wu, Lani F},
 month = jul,
 year = {2012},
 pages = {634--635},
}

@article{trampert_deep_2021,
 title = {Deep {Neural} {Networks} for {Analysis} of {Microscopy} {Images}—{Synthetic} {Data} {Generation} and {Adaptive} {Sampling}},
 volume = {11},
 copyright = {https://creativecommons.org/licenses/by/4.0/},
 issn = {2073-4352},
 url = {https://www.mdpi.com/2073-4352/11/3/258},
 doi = {10.3390/cryst11030258},
 abstract = {The analysis of microscopy images has always been an important yet time consuming process in materials science. Convolutional Neural Networks (CNNs) have been very successfully used for a number of tasks, such as image segmentation. However, training a CNN requires a large amount of hand annotated data, which can be a problem for material science data. We present a procedure to generate synthetic data based on ad hoc parametric data modelling for enhancing generalization of trained neural network models. Especially for situations where it is not possible to gather a lot of data, such an approach is beneficial and may enable to train a neural network reasonably. Furthermore, we show that targeted data generation by adaptively sampling the parameter space of the generative models gives superior results compared to generating random data points.},
 language = {en},
 number = {3},
 urldate = {2024-09-07},
 journal = {Crystals},
 author = {Trampert, Patrick and Rubinstein, Dmitri and Boughorbel, Faysal and Schlinkmann, Christian and Luschkova, Maria and Slusallek, Philipp and Dahmen, Tim and Sandfeld, Stefan},
 month = mar,
 year = {2021},
 pages = {258},
}

@article{lehmussola_synthetic_2008,
 title = {Synthetic {Images} of {High}-{Throughput} {Microscopy} for {Validation} of {Image} {Analysis} {Methods}},
 volume = {96},
 copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
 issn = {0018-9219, 1558-2256},
 url = {http://ieeexplore.ieee.org/document/4567415/},
 doi = {10.1109/JPROC.2008.925490},
 number = {8},
 urldate = {2024-09-07},
 journal = {Proceedings of the IEEE},
 author = {Lehmussola, A. and Ruusuvuori, P. and Selinummi, J. and Rajala, T. and Yli-Harja, O.},
 month = aug,
 year = {2008},
 pages = {1348--1360},
}

@InProceedings{david_svoboda_generation_2012,
 author="Svoboda, David and Ulman, Vladim{\'i}r",
 editor="Campilho, Aur{\'e}lio and Kamel, Mohamed",
 title="Generation of Synthetic Image Datasets for Time-Lapse Fluorescence Microscopy",
 booktitle="Image Analysis and Recognition",
 year="2012",
 publisher="Springer Berlin Heidelberg",
 address="Berlin, Heidelberg",
 pages="473--482",
 abstract="In the field of biomedical image analysis, motion tracking and segmentation algorithms are important tools for time-resolved analysis of cell characteristics, events, and tracking. There are many algorithms in everyday use. Nevertheless, most of them is not properly validated as the ground truth (GT), which is a very important tool for the verification of image processing algorithms, is not naturally available. Many algorithms in this field of study are, therefore, validated only manually by an human expert. This is usually difficult, cumbersome and time consuming task, especially when single 3D image or even 3D image sequence is considered.",
 isbn="978-3-642-31298-4"
}

@InProceedings{david_svoboda_towards_2013,
 author="Svoboda, David and Ulman, Vladim{\'i}r",
 editor="Petrosino, Alfredo",
 title="Towards a Realistic Distribution of Cells in Synthetically Generated 3D Cell Populations",
 booktitle="Image Analysis and Processing -- ICIAP 2013",
 year="2013",
 publisher="Springer Berlin Heidelberg",
 address="Berlin, Heidelberg",
 pages="429--438",
 abstract="In fluorescence microscopy, the proper evaluation of image segmentation algorithms is still an open problem. In the field of cell segmentation, such evaluation can be seen as a study of the given algorithm how well it can discover individual cells as a function of the number of them in an image (size of cell population), their mutual positions (density of cell clusters), and the level of noise. Principally, there are two approaches to the evaluation. One approach requires real input images and an expert that verifies the segmentation results. This is, however, expert dependent and, namely when handling 3D data, very tedious. The second approach uses synthetic images with ground truth data to which the segmentation result is compared objectively. In this paper, we propose a new method for generating synthetic 3D images showing naturally distributed cell populations attached to microscope slide. Cell count and clustering probability are user parameters of the method.",
 isbn="978-3-642-41184-7"
}

@article{christiansen_silico_2018,
 title = {In {Silico} {Labeling}: {Predicting} {Fluorescent} {Labels} in {Unlabeled} {Images}},
 volume = {173},
 issn = {00928674},
 shorttitle = {In {Silico} {Labeling}},
 url = {https://linkinghub.elsevier.com/retrieve/pii/S0092867418303647},
 doi = {10.1016/j.cell.2018.03.040},
 language = {en},
 number = {3},
 urldate = {2024-06-07},
 journal = {Cell},
 author = {Christiansen, Eric M. and Yang, Samuel J. and Ando, D. Michael and Javaherian, Ashkan and Skibinski, Gaia and Lipnick, Scott and Mount, Elliot and O’Neil, Alison and Shah, Kevan and Lee, Alicia K. and Goyal, Piyush and Fedus, William and Poplin, Ryan and Esteva, Andre and Berndl, Marc and Rubin, Lee L. and Nelson, Philip and Finkbeiner, Steven},
 month = apr,
 year = {2018},
 pages = {792--803.e19},
}

@article{gyuhyun_lee_deephcs_2018,
 title = {{DeepHCS}: {Bright}-{Field} to {Fluorescence} {Microscopy} {Image} {Conversion} {Using} {Deep} {Learning} for {Label}-{Free} {High}-{Content} {Screening}},
 doi = {10.1007/978-3-030-00934-2_38},
 abstract = {In this paper, we propose a novel image processing method, DeepHCS, to transform bright-field microscopy images into synthetic fluorescence images of cell nuclei biomarkers commonly used in high-content drug screening. The main motivation of the proposed work is to automatically generate virtual biomarker images from conventional bright-field images, which can greatly reduce time-consuming and laborious tissue preparation efforts and improve the throughput of the screening process. DeepHCS uses bright-field images and their corresponding cell nuclei staining (DAPI) fluorescence images as a set of image pairs to train a series of end-to-end deep convolutional neural networks. By leveraging a state-of-the-art deep learning method, the proposed method can produce synthetic fluorescence images comparable to real DAPI images with high accuracy. We demonstrate the efficacy of this method using a real glioblastoma drug screening dataset with various quality metrics, including PSNR, SSIM, cell viability correlation (CVC), the area under the curve (AUC), and the IC50.},
 journal = {International Conference on Medical Image Computing and Computer-Assisted Intervention},
 author = {{Gyuhyun Lee} and Lee, Gyuhyun and {Jeong-Woo Oh} and Oh, Jeong-Woo and {Mi-Sun Kang} and Kang, Mi-Sun and {Nam-Gu Her} and Her, Nam-Gu and {Myoung-Hee Kim} and Kim, Myoung-Hee and {Won-Ki Jeong} and Jeong, Won-Ki},
 month = sep,
 year = {2018},
 doi = {10.1007/978-3-030-00934-2_38},
 note = {MAG ID: 2892296896
S2ID: 6221dc0f12e1513610a3a4de8b1fc4adff628825},
 pages = {335--343},
}

@article{gyuhyun_lee_deephcs_2021,
 title = {{DeepHCS}++: {Bright}-field to fluorescence microscopy image conversion using multi-task learning with adversarial losses for label-free high-content screening.},
 volume = {70},
 doi = {10.1016/j.media.2021.101995},
 abstract = {Abstract In this paper, we propose a novel microscopy image translation method for transforming a bright-field microscopy image into three different fluorescence images to observe the apoptosis, nuclei, and cytoplasm of cells, which visualize dead cells, nuclei of cells, and cytoplasm of cells, respectively. These biomarkers are commonly used in high-content drug screening to analyze drug response. The main contribution of the proposed work is the automatic generation of three fluorescence images from a conventional bright-field image; this can greatly reduce the time-consuming and laborious tissue preparation process and improve throughput of the screening process. Our proposed method uses only a single bright-field image and the corresponding fluorescence images as a set of image pairs for training an end-to-end deep convolutional neural network. By leveraging deep convolutional neural networks with a set of image pairs of bright-field and corresponding fluorescence images, our proposed method can produce synthetic fluorescence images comparable to real fluorescence microscopy images with high accuracy. Our proposed model uses multi-task learning with adversarial losses to generate more accurate and realistic microscopy images. We assess the efficacy of the proposed method using real bright-field and fluorescence microscopy image datasets from patient-driven samples of a glioblastoma, and validate the method’s accuracy with various quality metrics including cell number correlation (CNC), peak signal-to-noise ratio (PSNR), structural similarity index measure (SSIM), cell viability correlation (CVC), error maps, and R 2 correlation.},
 journal = {Medical Image Analysis},
 author = {{Gyuhyun Lee} and Lee, Gyuhyun and {Jeong-Woo Oh} and Oh, Jeong-Woo and Oh, Jeong-Woo and {Nam-Gu Her} and Her, Nam-Gu and {Won-Ki Jeong} and Jeong, Won-Ki},
 year = {2021},
 doi = {10.1016/j.media.2021.101995},
 pmid = {33640720},
 note = {MAG ID: 3133306532
S2ID: 6ba26fb5c73049bf60d365a19be4cc209c4f917e},
 pages = {101995},
}

@article{zhang_high-throughput_2019,
 title = {High-throughput, high-resolution deep learning microscopy based on registration-free generative adversarial network},
 volume = {10},
 issn = {2156-7085, 2156-7085},
 url = {https://opg.optica.org/abstract.cfm?URI=boe-10-3-1044},
 doi = {10.1364/BOE.10.001044},
 language = {en},
 number = {3},
 urldate = {2024-06-07},
 journal = {Biomedical Optics Express},
 author = {Zhang, Hao and Fang, Chunyu and Xie, Xinlin and Yang, Yicong and Mei, Wei and Jin, Di and Fei, Peng},
 month = mar,
 year = {2019},
 pages = {1044},
}

@article{marin_scalbert_generic_2019,
 title = {Generic {Isolated} {Cell} {Image} {Generator}.},
 volume = {95},
 doi = {10.1002/cyto.a.23899},
 abstract = {Building automated cancer screening systems based on image analysis is currently a hot topic in computer vision and medical imaging community. One of the biggest challenges of such systems, especially those using state‐of‐the‐art deep learning techniques, is that they usually require a large amount of training data to be accurate. However, in the medical field, the confidentiality of the data and the need for medical expertise to label them significantly reduce the amount of training data available. A common practice to overcome this problem is to apply data set augmentation techniques to artificially increase the size of the training data set. Classical data set augmentation methods such as geometrical or color transformations are efficient but still produce a limited amount of new data. Hence, there has been interest in data set augmentation methods using generative models able to synthesize a wider variety of new data. VitaDX is actually developing an automated bladder cancer screening system based on the analysis of cell images contained in urinary cytology digital slides. Currently, the number of available labeled cell images is limited and therefore exploitation of the full potential of deep learning techniques is not possible. In an attempt to increase the number of labeled cell images, a new generic generator for 2D cell images has been developed and is described in this article. This framework combines previous works on cell image generation and a recent style transfer method referred to as doodle‐style transfer in this article. To the best of our knowledge, we are the first to use a doodle‐style transfer method for synthetic cell image generation. This framework is quite modular and could be applied to other cell image generation problems. A statistical evaluation has shown that features of real and synthetic cell images followed roughly the same distribution. Finally, the realism of the synthetic cell images has been assessed through a visual evaluation performed with the help of medical experts. © 2019 The Authors. Cytometry Part A published by Wiley Periodicals, Inc. on behalf of International Society for Advancement of Cytometry.},
 number = {11},
 journal = {Cytometry Part A},
 author = {{Marin Scalbert} and Scalbert, Marin and {Florent Couzinié-Devy} and Couzinie‐Devy, Florent and {Riadh Fezzani} and Fezzani, Riadh},
 month = nov,
 year = {2019},
 doi = {10.1002/cyto.a.23899},
 pmcid = {6899488},
 note = {MAG ID: 2979953123},
}

@article{jascha_sohl-dickstein_deep_2015,
 title = {Deep {Unsupervised} {Learning} using {Nonequilibrium} {Thermodynamics}},
 abstract = {A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.},
 journal = {arXiv: Learning},
 author = {{Jascha Sohl-Dickstein} and Sohl-Dickstein, Jascha and {Eric A. Weiss} and Weiss, Eric L. and {Niru Maheswaranathan} and Maheswaranathan, Niru and {Surya Ganguli} and Ganguli, Surya},
 month = mar,
 year = {2015},
 note = {ARXIV\_ID: 1503.03585
MAG ID: 2129069237
S2ID: 2dcef55a07f8607a819c21fe84131ea269cc2e3c},
}

@article{song_denoising_2020,
 title = {Denoising {Diffusion} {Implicit} {Models}},
 copyright = {arXiv.org perpetual, non-exclusive license},
 url = {https://arxiv.org/abs/2010.02502},
 doi = {10.48550/ARXIV.2010.02502},
 journal = {CoRR},
 abstract = {Denoising diffusion probabilistic models (DDPMs) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps to produce a sample. To accelerate sampling, we present denoising diffusion implicit models (DDIMs), a more efficient class of iterative implicit probabilistic models with the same training procedure as DDPMs. In DDPMs, the generative process is defined as the reverse of a Markovian diffusion process. We construct a class of non-Markovian diffusion processes that lead to the same training objective, but whose reverse process can be much faster to sample from. We empirically demonstrate that DDIMs can produce high quality samples \$10 {\textbackslash}times\$ to \$50 {\textbackslash}times\$ faster in terms of wall-clock time compared to DDPMs, allow us to trade off computation for sample quality, and can perform semantically meaningful image interpolation directly in the latent space.},
 urldate = {2023-08-07},
 author = {Song, Jiaming and Meng, Chenlin and Ermon, Stefano},
 year = {2020},
 note = {Publisher: arXiv
Version Number: 4},
 keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, Machine Learning (cs.LG)},
}

@misc{cross-zamirski_class-guided_2023,
 title = {Class-{Guided} {Image}-to-{Image} {Diffusion}: {Cell} {Painting} from {Brightfield} {Images} with {Class} {Labels}},
 shorttitle = {Class-{Guided} {Image}-to-{Image} {Diffusion}},
 url = {http://arxiv.org/abs/2303.08863},
 abstract = {Image-to-image reconstruction problems with free or inexpensive metadata in the form of class labels appear often in biological and medical image domains. Existing text-guided or style-transfer image-to-image approaches do not translate to datasets where additional information is provided as discrete classes. We introduce and implement a model which combines image-to-image and class-guided denoising diffusion probabilistic models. We train our model on a real-world dataset of microscopy images used for drug discovery, with and without incorporating metadata labels. By exploring the properties of image-to-image diffusion with relevant labels, we show that class-guided image-to-image diffusion can improve the meaningful content of the reconstructed images and outperform the unguided model in useful downstream tasks.},
 urldate = {2023-08-07},
 publisher = {arXiv},
 author = {Cross-Zamirski, Jan Oscar and Anand, Praveen and Williams, Guy and Mouchet, Elizabeth and Wang, Yinhai and Schönlieb, Carola-Bibiane},
 month = mar,
 year = {2023},
 note = {arXiv:2303.08863 [cs, q-bio]},
 keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Quantitative Biology - Quantitative Methods},
}

@article{gabriel_della_maggiora_conditional_2023,
 title = {Conditional {Variational} {Diffusion} {Models}},
 doi = {10.48550/arxiv.2312.02246},
 abstract = {Inverse problems aim to determine parameters from observations, a crucial task in engineering and science. Lately, generative models, especially diffusion models, have gained popularity in this area for their ability to produce realistic solutions and their good mathematical properties. Despite their success, an important drawback of diffusion models is their sensitivity to the choice of variance schedule, which controls the dynamics of the diffusion process. Fine-tuning this schedule for specific applications is crucial but time-costly and does not guarantee an optimal result. We propose a novel approach for learning the schedule as part of the training process. Our method supports probabilistic conditioning on data, provides high-quality solutions, and is flexible, proving able to adapt to different applications with minimum overhead. This approach is tested in two unrelated inverse problems: super-resolution microscopy and quantitative phase imaging, yielding comparable or superior results to previous methods and fine-tuned diffusion models. We conclude that fine-tuning the schedule by experimentation should be avoided because it can be learned during training in a stable way that yields better results.},
 journal = {arXiv.org},
 author = {{Gabriel della Maggiora} and {L. A. Croquevielle} and {Nikita Desphande} and {Harry Horsley} and {Thomas Heinis} and {Artur Yakimovich}},
 year = {2023},
 doi = {10.48550/arxiv.2312.02246},
 note = {ARXIV\_ID: 2312.02246
S2ID: 0aa948324bca48784d2a78a799c92b101687c89e},
}

@article{lu_diffusion-based_2024,
 title = {Diffusion-based deep learning method for augmenting ultrastructural imaging and volume electron microscopy},
 volume = {15},
 issn = {2041-1723},
 url = {https://www.nature.com/articles/s41467-024-49125-z},
 doi = {10.1038/s41467-024-49125-z},
 abstract = {Abstract
 Electron microscopy (EM) revolutionized the way to visualize cellular ultrastructure. Volume EM (vEM) has further broadened its three-dimensional nanoscale imaging capacity. However, intrinsic trade-offs between imaging speed and quality of EM restrict the attainable imaging area and volume. Isotropic imaging with vEM for large biological volumes remains unachievable. Here, we developed EMDiffuse, a suite of algorithms designed to enhance EM and vEM capabilities, leveraging the cutting-edge image generation diffusion model. EMDiffuse generates realistic predictions with high resolution ultrastructural details and exhibits robust transferability by taking only one pair of images of 3 megapixels to fine-tune in denoising and super-resolution tasks. EMDiffuse also demonstrated proficiency in the isotropic vEM reconstruction task, generating isotropic volume even in the absence of isotropic training data. We demonstrated the robustness of EMDiffuse by generating isotropic volumes from seven public datasets obtained from different vEM techniques and instruments. The generated isotropic volume enables accurate three-dimensional nanoscale ultrastructure analysis. EMDiffuse also features self-assessment functionalities on predictions’ reliability. We envision EMDiffuse to pave the way for investigations of the intricate subcellular nanoscale ultrastructure within large volumes of biological systems.},
 language = {en},
 number = {1},
 urldate = {2024-06-07},
 journal = {Nature Communications},
 author = {Lu, Chixiang and Chen, Kai and Qiu, Heng and Chen, Xiaojun and Chen, Gu and Qi, Xiaojuan and Jiang, Haibo},
 month = jun,
 year = {2024},
 pages = {4677},
}

@misc{li_microscopy_2023,
 title = {Microscopy image reconstruction with physics-informed denoising diffusion probabilistic model},
 copyright = {Creative Commons Attribution 4.0 International},
 url = {https://arxiv.org/abs/2306.02929},
 doi = {10.48550/ARXIV.2306.02929},
 abstract = {Light microscopy is a widespread and inexpensive imaging technique facilitating biomedical discovery and diagnostics. However, light diffraction barrier and imperfections in optics limit the level of detail of the acquired images. The details lost can be reconstructed among others by deep learning models. Yet, deep learning models are prone to introduce artefacts and hallucinations into the reconstruction. Recent state-of-the-art image synthesis models like the denoising diffusion probabilistic models (DDPMs) are no exception to this. We propose to address this by incorporating the physical problem of microscopy image formation into the model's loss function. To overcome the lack of microscopy data, we train this model with synthetic data. We simulate the effects of the microscope optics through the theoretical point spread function and varying the noise levels to obtain synthetic data. Furthermore, we incorporate the physical model of a light microscope into the reverse process of a conditioned DDPM proposing a physics-informed DDPM (PI-DDPM). We show consistent improvement and artefact reductions when compared to model-based methods, deep-learning regression methods and regular conditioned DDPMs.},
 urldate = {2024-06-07},
 publisher = {arXiv},
 author = {Li, Rui and della Maggiora, Gabriel and Andriasyan, Vardan and Petkidis, Anthony and Yushkevich, Artsemi and Kudryashev, Mikhail and Yakimovich, Artur},
 year = {2023},
 note = {Version Number: 2},
 keywords = {FOS: Biological sciences, I.4; J.3, Quantitative Methods (q-bio.QM)},
}

@article{juan_c_caicedo_evaluation_2019,
 title = {Evaluation of {Deep} {Learning} {Strategies} for {Nucleus} {Segmentation} in {Fluorescence} {Images}},
 volume = {95},
 doi = {10.1002/cyto.a.23863},
 abstract = {Identifying nuclei is often a critical first step in analyzing microscopy images of cells and classical image processing algorithms are most commonly used for this task. Recent developments in deep learning can yield superior accuracy, but typical evaluation metrics for nucleus segmentation do not satisfactorily capture error modes that are relevant in cellular images. We present an evaluation framework to measure accuracy, types of errors, and computational efficiency; and use it to compare deep learning strategies and classical approaches. We publicly release a set of 23,165 manually annotated nuclei and source code to reproduce experiments and run the proposed evaluation methodology. Our evaluation framework shows that deep learning improves accuracy and can reduce the number of biologically relevant errors by half. © 2019 The Authors. Cytometry Part A published by Wiley Periodicals, Inc. on behalf of International Society for Advancement of Cytometry.},
 number = {9},
 journal = {Cytometry Part A},
 author = {{Juan C Caicedo} and Caicedo, Juan C. and {Jonathan Roth} and Roth, Jonathan R. and {Allen Goodman} and Goodman, Allen and {Tim Becker} and Becker, Tim and {Tim Becker} and {Kyle W. Karhohs} and Karhohs, Kyle W. and {Matthieu Broisin} and Broisin, Matthieu and {Csaba Molnár} and Molnar, Csaba and {Claire McQuin} and McQuin, Claire and {Shantanu Singh} and Singh, Shantanu and {Fabian J. Theis} and Theis, Fabian J. and {Anne E. Carpenter} and Carpenter, Anne E.},
 month = jul,
 year = {2019},
 doi = {10.1002/cyto.a.23863},
 note = {MAG ID: 2961912654},
 pages = {952--965},
}

@article{ricard_durall_combating_2020,
 title = {Combating {Mode} {Collapse} in {GAN} training: {An} {Empirical} {Analysis} using {Hessian} {Eigenvalues}},
 doi = {10.5220/0010167902110218},
 abstract = {Generative adversarial networks (GANs) provide state-of-the-art results in image generation. However, despite being so powerful, they still remain very challenging to train. This is in particular caused by their highly non-convex optimization space leading to a number of instabilities. Among them, mode collapse stands out as one of the most daunting ones. This undesirable event occurs when the model can only fit a few modes of the data distribution, while ignoring the majority of them. In this work, we combat mode collapse using second-order gradient information. To do so, we analyse the loss surface through its Hessian eigenvalues, and show that mode collapse is related to the convergence towards sharp minima. In particular, we observe how the eigenvalues of the \$G\$ are directly correlated with the occurrence of mode collapse. Finally, motivated by these findings, we design a new optimization algorithm called nudged-Adam (NuGAN) that uses spectral information to overcome mode collapse, leading to empirically more stable convergence properties.},
 journal = {arXiv: Learning},
 author = {{Ricard Durall} and Durall, Ricard and {Avraam Chatzimichailidis} and Chatzimichailidis, Avraam and {Peter Labus} and {Peter Labus} and {Peter Labus} and Labus, Peter and {Janis Keuper} and Keuper, Janis},
 year = {2020},
 doi = {10.5220/0010167902110218},
 note = {ARXIV\_ID: 2012.09673
MAG ID: 3112865286
S2ID: 3ad13bd6713d74eec7faa964050f7d61089440a0},
}

@misc{jocher_ultralytics_2023,
 title = {Ultralytics {YOLO}},
 copyright = {AGPL-3.0},
 url = {https://github.com/ultralytics/ultralytics},
 abstract = {NEW - YOLOv8 in PyTorch {\textgreater} ONNX {\textgreater} OpenVINO {\textgreater} CoreML {\textgreater} TFLite},
 urldate = {2024-08-13},
 author = {Jocher, Glenn and Chaurasia, Ayush and Qiu, Jing},
 month = jan,
 year = {2023},
 note = {original-date: 2022-09-11T16:39:45Z},
}

@article{Wang2024YOLOv9LW,
 title = {{YOLOv9}: {Learning} what you want to learn using programmable gradient information},
 volume = {abs/2402.13616},
 url = {https://api.semanticscholar.org/CorpusID:267770251},
 journal = {ArXiv},
 author = {Wang, Chien-Yao and Yeh, I-Hau and Liao, Hongpeng},
 year = {2024},
}

@article{Lv2023DETRsBY,
 title = {{DETRs} beat yolos on real-time object detection},
 volume = {abs/2304.08069},
 url = {https://api.semanticscholar.org/CorpusID:258179840},
 journal = {ArXiv},
 author = {Lv, Wenyu and Xu, Shangliang and Zhao, Yian and Wang, Guanzhong and Wei, Jinman and Cui, Cheng and Du, Yuning and Dang, Qingqing and Liu, Yi},
 year = {2023},
}

@article{tsung-yi_lin_microsoft_2014,
 author       = {Tsung{-}Yi Lin and
                  Michael Maire and
                  Serge J. Belongie and
                  Lubomir D. Bourdev and
                  Ross B. Girshick and
                  James Hays and
                  Pietro Perona and
                  Deva Ramanan and
                  Piotr Doll{\'{a}}r and
                  C. Lawrence Zitnick},
 title        = {Microsoft {COCO:} Common Objects in Context},
 journal      = {CoRR},
 volume       = {abs/1405.0312},
 year         = {2014},
 url          = {http://arxiv.org/abs/1405.0312},
 eprinttype    = {arXiv},
 eprint       = {1405.0312},
 timestamp    = {Mon, 13 Aug 2018 16:48:13 +0200},
 biburl       = {https://dblp.org/rec/journals/corr/LinMBHPRDZ14.bib},
 bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{f_mualla_automatic_2016,
 title = {Automatic {Unstained} {Cell} {Detection} in {Bright} {Field} {Microscopy} ({Automatische} {Detektion} ungefärbter {Zellen} in der {Hellfeld}-{Mikroskopie})},
 author = {{F. Mualla}},
 year = {2016},
 note = {S2ID: 63b5dcdf97b4b69d5112d0e060c79cb6a3ad7064},
}

@article{e_d_ferreira_classification_2024,
 title = {Classification and counting of cells in brightfield microscopy images: an application of convolutional neural networks},
 doi = {10.1038/s41598-024-59625-z},
 journal = {Scientific Reports},
 author = {{E. D. Ferreira} and {G. F. Silveira}},
 year = {2024},
 doi = {10.1038/s41598-024-59625-z},
 pmcid = {11031575},
 pmid = {38641688},
 note = {S2ID: 27078ac4ab60c12c1c98e9aab000c6b3673eff22},
}

@article{huixia_ren_cellbow_2020,
 title = {Cellbow: a robust customizable cell segmentation program},
 volume = {8},
 doi = {10.1007/s40484-020-0213-6},
 abstract = {Time-lapse live cell imaging of a growing cell population is routine in many biological investigations. A major challenge in imaging analysis is accurate segmentation, a process to define the boundaries of cells based on raw image data. Current segmentation methods relying on single boundary features have problems in robustness when dealing with inhomogeneous foci which invariably happens in cell population imaging. Combined with a multi-layer training set strategy, we developed a neural-network-based algorithm — Cellbow. Cellbow can achieve accurate and robust segmentation of cells in broad and general settings. It can also facilitate long-term tracking of cell growth and division. To facilitate the application of Cellbow, we provide a website on which one can online test the software, as well as an ImageJ plugin for the user to visualize the performance before software installation. Cellbow is customizable and generalizable. It is broadly applicable to segmenting fluorescent images of diverse cell types with no further training needed. For bright-field images, only a small set of sample images of the specific cell type from the user may be needed for training.},
 number = {3},
 author = {{Huixia Ren} and {Huixia Ren} and Ren, Huixia and Ren, Huixia and {Mengdi Zhao} and Zhao, Mengdi and {Bo Liu} and {Bo Liu} and {Bo Liu} and Liu, Bo and {Ruixiao Yao} and Yao, Ruixiao and {Ruixiao Yao} and {Ruixiao Yao} and {Lei Qi} and {Qi Liu} and Liu, Qi and {Zhipeng Ren} and Ren, Zhipeng and {Zirui Wu} and Wu, Zirui and {Zongmao Gao} and Gao, Zongmao and {Xiu Yang} and Yang, Xiaojing and {Chao Tang} and Tang, Chao},
 year = {2020},
 doi = {10.1007/s40484-020-0213-6},
 note = {MAG ID: 3083174864},
 pages = {245--255},
}

@misc{falcon_pytorchlightningpytorch-lightning_2020,
 title = {{PyTorchLightning}/pytorch-lightning: 0.7.6 release},
 copyright = {Open Access},
 shorttitle = {{PyTorchLightning}/pytorch-lightning},
 url = {https://zenodo.org/record/3828935},
 abstract = {The lightweight PyTorch wrapper for ML researchers. Scale your models. Write less boilerplate},
 urldate = {2024-07-31},
 publisher = {Zenodo},
 author = {Falcon, William and Borovec, Jirka and Wälchli, Adrian and Eggert, Nic and Schock, Justus and Jordan, Jeremy and Skafte, Nicki and {Ir1dXD} and Bereznyuk, Vadim and Harris, Ethan and {Tullie Murrell} and Yu, Peter and Præsius, Sebastian and Addair, Travis and Zhong, Jacob and Lipin, Dmitry and Uchida, So and {Shreyas Bapat} and Schröter, Hendrik and Dayma, Boris and Karnachev, Alexey and {Akshay Kulkarni} and {Shunta Komatsu} and {Martin.B} and {Jean-Baptiste SCHIRATTI} and Mary, Hadrien and Byrne, Donal and {Cristobal Eyzaguirre} and {Cinjon} and Bakhtin, Anton},
 month = may,
 year = {2020},
 doi = {10.5281/ZENODO.3828935},
}

@incollection{vohra_apache_2016,
 address = {Berkeley, CA},
 title = {Apache {Parquet}},
 copyright = {http://www.springer.com/tdm},
 isbn = {978-1-4842-2198-3 978-1-4842-2199-0},
 url = {http://link.springer.com/10.1007/978-1-4842-2199-0_8},
 language = {en},
 urldate = {2024-07-03},
 booktitle = {Practical {Hadoop} {Ecosystem}},
 publisher = {Apress},
 author = {Vohra, Deepak},
 collaborator = {Vohra, Deepak},
 year = {2016},
 doi = {10.1007/978-1-4842-2199-0_8},
 pages = {325--335},
}

@misc{dwyer_roboflow_nodate,
 title = {Roboflow ({Version} 1.0)},
 url = {https://roboflow.com},
 publisher = {Roboflow Inc.},
 author = {Dwyer, B. and Nelson, J. and Hansen, T.},
}

@article{gregory_pappas_exploring_2005,
 title = {Exploring ethical considerations for the use of biological and physiological markers in population-based surveys in less developed countries},
 volume = {1},
 doi = {10.1186/1744-8603-1-16},
 abstract = {Background
The health information needs of developing countries increasingly include population-based estimates determined by biological and physiological measures. Collection of data on these biomarkers requires careful reassessment of ethical standards and procedures related to issues of safety, informed consent, reporting, and referral policies. This paper reviews the survey practices of health examination surveys that have been conducted in developed nations and discusses their application to similar types of surveys proposed for developing countries.},
 number = {1},
 journal = {Globalization and Health},
 author = {{Gregory Pappas} and Pappas, Gregory and {Adnan A. Hyder} and Hyder, Adnan A.},
 month = nov,
 year = {2005},
 doi = {10.1186/1744-8603-1-16},
 pmcid = {1315319},
 pmid = {16313670},
 note = {MAG ID: 2137103412},
 pages = {16--16},
}

@article{i_galende_ethical_2023,
 title = {Ethical considerations about the collection of biological samples for genetic analysis in clinical trials},
 volume = {19},
 doi = {10.1177/17470161231152077},
 abstract = {Progress in precision medicine is being achieved through the design of clinical trials that use genetic biomarkers to guide stratification of patients and assignation to treatment or control groups. Genetic analysis of biomarkers is, therefore, essential to complete their objectives, and this involves the study of biological samples from donor patients that have been recruited according to criteria previously established in the design of the clinical trial. Nevertheless, it is becoming very common that, in the solicitation of biological samples, purposes that are beyond the objectives of the stated therapeutic trial research are introduced, like the development of ill-explained exploratory studies or the use in unspecified future research. In the digital era, the sequencing of patients’ DNA needs to be considered as a serious security matter, not only for the patients, but also for their relatives. Genetic information may be easily stored, even forever, in digital files. This engenders a permanent risk of being stolen or misused in many ways. Furthermore, re-identification of sample donors is technically feasible through their genetic data. For these reasons, genetic analysis of samples collected in clinical trials should be restricted to the accomplishment of their main objectives or well justified goals.},
 number = {2},
 journal = {Research Ethics},
 author = {{I Galende} and {Octavio Miguel Rivero-Lezcano}},
 month = jan,
 year = {2023},
 doi = {10.1177/17470161231152077},
 note = {MAG ID: 4319788163
S2ID: afcea223cd5d53938877055d1befd4126f0c19b6},
 pages = {220--226},
}

@article{anne_cambonthomsen_series_2007,
 title = {{SERIES} ''{GENETICS} {OF} {ASTHMA} {AND} {COPD} {IN} {THE} {POSTGENOME} {ERA}'' {Edited} by {E}. von {Mutius}, {M}. {Kabesch} and {F}. {Kauffmann} {Number} 6 in this {Series} {Trends} in ethical and legal frameworks for the use of human biobanks},
 doi = {10.1183/09031936.00165006},
 abstract = {Numerous studies of genetic epidemiology and post-genomics in respiratory diseases rely on the use of biobanks, defined as organised biological sample collections with associated personal and clinical data. The use of biobanks is increasing and raises several ethical issues. What are the ethical trends and legal frameworks in the post-genomic era? Are there new issues in relation to the developments of techniques and new study designs? How does this affect the clinician's attitudes and relationship with the patients? The main ethical issues encountered are: informed consent; confidentiality; secondary use of samples and data over time; return of results; and data sharing. Different levels and modalities of dealing with such issues are identified and vary from legally binding measures to ''soft'' regulations, such as ethical recommendations by various committees or professional organisa- tions. A further level of complexity appears with the increasing international dimension of such activities in a context in which national positions vary on those topics. There is a tension between a necessary level of diversity in ethical positions and an indispensable common pedestal of principles and procedures to manage these issues in order to foster research. Current legal and ethical trends favour the facilitation of secondary use of samples, more biobank openness, balanced with a growing attention to dialogue and public/stakeholder consultation, an increased role for research ethics committees and more sophisticated data protection and governance structures.},
 journal = {European Respiratory Journal},
 author = {{Anne Cambon‐Thomsen} and Cambon-Thomsen, A. and {Emmanuelle Rial‐Sebbag} and Rial-Sebbag, E.},
 month = jan,
 year = {2007},
 doi = {10.1183/09031936.00165006},
 pmid = {17666560},
 note = {MAG ID: 2186909180
S2ID: e744f40a504c1a2211549676a42a81c33a3faf65},
}